{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85d6e644-627e-45c7-949d-23d11f6fdc61",
    "_uuid": "3eb06d0b1e796353a1eb1a787809d6d79e8ed8fe"
   },
   "source": [
    "# Model Fit Metrics\n",
    "\n",
    "Once we've built a model, it's important to understand how well it works. To do so, we evaluate the model against one or more metrics. This notebook is an overview of some of the most common metrics used for regression models.\n",
    "\n",
    "We'll implement the metrics and test them out on a mocked-up regression target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "223d8f8f-4e5b-4ce9-bacf-45ecb2679172",
    "_uuid": "2e8f2c9326b0f2d6a4356789b6485be44fa9f175"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "clf = LinearRegression()\n",
    "\n",
    "np.random.seed(42)\n",
    "X = (np.array(sorted(list(range(5))*20)) + np.random.normal(size=100, scale=0.5))[:, \n",
    "                                                                                  np.newaxis]\n",
    "y = (np.array(sorted(list(range(5))*20)) + np.random.normal(size=100, scale=0.25))[:, \n",
    "                                                                                   np.newaxis]\n",
    "\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59998d92-5794-40c9-9a2b-a265567625db",
    "_uuid": "e10b48cb2ba4b7121a7372b92c47bf40693cb1c9",
    "collapsed": true
   },
   "source": [
    "## $R^2$\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The first and most immediately useful metric to use in regression is the $R^2$, also known as the coefficient of determination. For a vector of values $y$, a vector of predictions $\\hat{y}$, both of length $n$, and a value average $\\bar{y}$, $R^2$ is determined by:\n",
    "\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_0^{n-1} (y_i - \\hat{y}_i)^2}{\\sum_0^{n-1}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "The coefficient of determination is a measure of how well future samples will be predicted by the model. The best possible score is 1. A constant model which always predicts the average will recieve a score of 0. A model which is arbitrarily worse than an averaging model will recieve a negative score (this shouldn't happen in practice obviously!).\n",
    "\n",
    "In practice, it is a \"best default\" model score: other metrics may be better to use, depending on what you are optimizing for, but the $R^2$ is just generally very good, and should be the first number you look at in most cases.\n",
    "\n",
    "$R^2$ is such a popular metric that there are artificial $R^2$ scores, designed to work in a similar way but with completely different underlying mathematics, which are defined for other non-regression operations.\n",
    "\n",
    "### Hand Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "51aa3413-8fb2-4186-b1c5-b7b6ca8109c0",
    "_uuid": "3327b7d04f592c996c9f2fb3e1e7992178dec8fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98465584]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def squared_error(y1,y2):\n",
    "    return sum((y1 - y2) * (y1 - y2))\n",
    "\n",
    "def r2_score(y, y_pred):\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    squared_error_regr = squared_error(y, y_pred)\n",
    "    squared_error_y_mean = squared_error(y, y.mean())\n",
    "    output = 1 - (squared_error_regr/squared_error_y_mean)\n",
    "\n",
    "    print(output)\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d2d0e9f-5782-4f5b-93f2-c61c326021bb",
    "_uuid": "34997a032079f5aad6690040b001ca97235f5d56"
   },
   "source": [
    "### Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1d329b6e-1135-4ac9-99ae-8c302ba681fc",
    "_uuid": "d39ab0b34a55351b8e1fae1515fcce0205647079"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846558399170495"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6e65de0-5cea-4708-ba9e-740bc964f424",
    "_uuid": "4d468e2d81fac8dd786924cd7dde325a48d28720"
   },
   "source": [
    "## Residual Sum of Squares (RSS)\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The residual sum of squares is the top term in the $R^2$ metric (albeit adjusted by 1 to account for degrees of freedom). It takes the distance between observed and predicted values (the residuals), squares them, and sums them all together. Ordinary least squares regression is designed to minimize exactly *this* value.\n",
    "\n",
    "$$\\text{RSS} = \\sum_0^{n - 1} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "RSS is not very interpretable on its own, because it is the sum of many (potentially very large) residuals. For this reason it is rarely used as a metric, but because it is so important to regression, it's often included in statistical fit assays.\n",
    "\n",
    "### Hand Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b30fd2ad-bcf1-41db-bc9f-24cfa1c6c024",
    "_uuid": "0c5be6cc74f8826d57409ad64e91e1efdf957997"
   },
   "outputs": [],
   "source": [
    "def squared_error(y1,y2):\n",
    "    return sum((y1 - y2) * (y1 - y2))\n",
    "\n",
    "def rss_score(y, y_pred):\n",
    "    \n",
    "     #### YOUR CODE HERE ###\n",
    "        \n",
    "    output = squared_error(y, y_pred)\n",
    "    print(output)\n",
    "\n",
    "   #### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "3ffa2769-f8b7-4659-a7fe-9319334198cf",
    "_uuid": "6e1754657184840dacea2a451472f5017d90e30d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.14741858]\n"
     ]
    }
   ],
   "source": [
    "rss_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e9f0aab-615a-4ac4-b97d-ba5ac8cb0e81",
    "_uuid": "87d1a331f289a3d00fc0f6ab0f7f21abdb9b7768"
   },
   "source": [
    "There is no `scikit-learn` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10ede6d4-8442-4893-8bbe-4c054854f6e2",
    "_uuid": "bccda872c32806ec93a321dac36a111576fe3cc9"
   },
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Mean squared error is the interpretable version of RSS. MSE divides RSS (again adjusted be 1, to account for degrees of freedom) by the number of samples in the dataset to arrive at the average amount of squared error in the model:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\cdot \\sum_0^{n - 1} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "This is easily interpretable, because it makes a lot of intrinsic sense. Ordinary least squares regression asks that we minimize quadratic error; MSE measures, on average, how much such error is left in the model. However, due to the squaring involved, it is not very robust against outliers.\n",
    "\n",
    "### Hand Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4d1b265d-3791-4839-aa38-672d50d5c11a",
    "_uuid": "e138a239acdf8f70553f788523a2abd771df1c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "[0.03147419]\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y, y_pred):\n",
    "    ##### YOUR CODE HERE #####\n",
    "    \n",
    "    output = (1/y.shape[0]) * squared_error(y, y_pred)\n",
    "    print(output)\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "495c1e93-d89d-4d25-bad9-10c2e08c3c80",
    "_uuid": "6ef9ffe6d9aa6a34187a2efb45222e3b8b333118"
   },
   "source": [
    "### Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b749fc0a-c15b-4e2a-8d8d-2ca87d39dd1a",
    "_uuid": "92b28900f503d3cef346bad486f94682addb7c62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03147418578949139"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06554500-db02-4589-a300-bdc99204e112",
    "_uuid": "c5de41c9d5cef7f8c7c7d7ccfcd555b9a6521045"
   },
   "source": [
    "## Mean Absolute Error\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Mean absolute error computes the expected absolute error (or [L1-norm loss](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms)). Because it involves means, not squared residuals, mean absolute error is more resistant to outliers than MSE is.\n",
    "\n",
    "$$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n}\\sum_0^{n-1} | y_i - \\hat{y}_i |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5eb08880-4176-481d-8996-d14b1fa29e35",
    "_uuid": "a19802735c9e6f99e3ae05cd2e271a31803266fc"
   },
   "source": [
    "### Hand implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ebed743e-7b55-4d8f-97d1-81a831afb537",
    "_uuid": "b124f4675eb1360bba7f2179feae96641e85f004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15371923]\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error(y, y_pred):\n",
    "    ##### YOUR CODE HERE #####\n",
    "    \n",
    "    MAE = sum((1/y.shape[0]) * abs(y-y_pred))\n",
    "    print(MAE)\n",
    "    \n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7671d01d-83ac-4075-8d85-197b5b289cb6",
    "_uuid": "69179640c0227a6c5b2a21282533cd430e9d1b2f"
   },
   "source": [
    "### Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "512ca95c-e3b3-4c9a-9031-a6b89b687bc6",
    "_uuid": "63a14cdaac189a4e1eba53d5637803f4c5b5fd2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15371923162949003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8453d4e-741a-4b3f-b0cd-5c25fbef3193",
    "_uuid": "f49ed0035ad6845a1557819b0166adfae429988c"
   },
   "source": [
    "## Median Absolute Error\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Mean absolute error computes the median absolute error. Because this value is not only an absolute value, but also a median instead of a mode, this metric is the most resistant metric to outliers that's possible using simple methods.\n",
    "\n",
    "$$\\text{Mean Absolute Error} = \\text{median}(|y_0 - \\hat{y}_0, \\ldots, |y_n - \\hat{y}_n|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41dbad57-802d-4be6-b1b2-3051c829d536",
    "_uuid": "a3f19b3a2e81f8d8f0ad6298cac9169c5c3ba45f"
   },
   "source": [
    "### Hand implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "cfa12eb2-e414-4a5d-953b-a72e1720d625",
    "_uuid": "27ad3436e1dff2a920f720713cfbc90099416420"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15371923162949003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def median_absolute_error(y, y_pred):\n",
    "    #### YOUR CODE HERE #### \n",
    "    val = []\n",
    "    for i in y.size:\n",
    "        val.append(y[i]-y_pred[i]) \n",
    "    \n",
    "    arr = np.array(val)\n",
    "    MAE = median(arr)\n",
    "    print(MAE)\n",
    "    #### END YOUR CODE ####\n",
    "    \n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f978d9c2-9105-4ed0-b034-d5624d1e8903",
    "_uuid": "5fd0c97e454e432c4309775b51c4121cb64e0b76"
   },
   "source": [
    "### Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "376b3dd6-f3f9-4866-8a8c-e83c305cd99d",
    "_uuid": "95925a3f42c1b64bd2de51da222483bbaeeef832"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15514813323997423"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "median_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c63e96d-24e6-4b32-ac63-5fd370b30878",
    "_uuid": "289e05643a74192b72b0d2144dcc752a032c0a6f"
   },
   "source": [
    "## Root mean squared error (RMSE)\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Root mean squared error is an error metric that's popular in the literature. It is defined as the square root of mean squared error:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n}\\sum_0^{n - 1} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "Since this is just the root of the MSE metric mentioned earlier, we will omit an implementation.\n",
    "\n",
    "RMSE is directly comparable to, and serves a similar role as, the MAE, mean absolute error. The difference between the two computationally speaking is that MAE takes the square root of the distance inside the sum, while RMSE takes the square root outside the sum.\n",
    "\n",
    "The computational effect is that RMSE is less resistant to outliers, and thus reports a poorer-fitting model when outliers are not properly accounted for. This is considered a good thing when doing cetain things, like performing hyperparameter searches. However, MAE is a more useful reporting statistic because MAE is *interpretable*, while RMSE is not.\n",
    "\n",
    "Context for this comparison [here](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d).\n",
    "\n",
    "Note that `scikit-learn` doesn't provide a RMSE evaluator directly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0270d3c3-2f90-482f-8c56-32fb4c88cec8",
    "_uuid": "5124d682a6dcad6fea9b4d9fa0b44244218b8eba"
   },
   "source": [
    "## Explained variance score\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The explained variance score is a very clever (IMO) metric which looks at the ratio between the variance of the model/truth differences and the variance of the ground truth alone:\n",
    "\n",
    "$$\\text{explained variance}(y, \\hat{y}) = 1 - \\frac{Var({y}) - Var(\\hat{y})}{Var(y)}$$\n",
    "\n",
    "Hence the moniker \"explained variance\". The best possible score is 1 (all variance is explained) and the score goes down from there. A further reference on explained variance is [here](https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f3ffb79-7765-4e94-a010-fd8479e94bb6",
    "_uuid": "6e925bf5ea21c80699e817a09570900e53ad9be4"
   },
   "source": [
    "### Hand implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "26530a33-bbb3-450b-96e7-190cc3769f62",
    "_uuid": "e86fc8d2e7bfc86fe38fc5f7713df79047116e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7766702313432963\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "\n",
    "def explained_variance_score(y, y_pred):\n",
    "    ###### YOUR CODE HERE ######\n",
    "    print()\n",
    "    varience = 1-(st.variance(y.flatten()) - st.variance(y_pred.flatten()) )/st.variance(y.flatten())\n",
    "    print(varience)\n",
    "    ###### END YOUR CODE ######\n",
    "\n",
    "explained_variance_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0908ccca-7b3b-4038-aeaa-4a0d4a0980b3",
    "_uuid": "b6bad04c4282baa3e38187f79447947becdf0cf8"
   },
   "source": [
    "### Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "9dd44f71-6672-4dad-837b-082124b3d117",
    "_uuid": "e1517e99e362a7d06cd562f2572039b6a5168200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859076890259523"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "explained_variance_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus:\n",
    "\n",
    "Open [Catalyst Metrics](https://catalyst-team.github.io/catalyst/api/metrics.html#runner-metrics) Implement the following metrics: \n",
    "\n",
    "* MRRMetric\n",
    "* FunctionalBatchMetric\n",
    "* TopKMetric\n",
    "\n",
    "\n",
    "If you are not able to implement these metrics, explain why and how you would implement them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.), tensor(1.)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from catalyst import metrics\n",
    "\n",
    "\n",
    "metrics.mrr(\n",
    "    outputs=torch.Tensor(y_pred),\n",
    "    targets=torch.Tensor(y),\n",
    "    topk=[1, 3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m metric \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mFunctionalBatchMetric(\n\u001b[0;32m     11\u001b[0m     metric_fn\u001b[38;5;241m=\u001b[39msklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maccuracy_score,\n\u001b[0;32m     12\u001b[0m     metric_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m metric\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m metric\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     19\u001b[0m metric\u001b[38;5;241m.\u001b[39mcompute_key_value()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GSU\\lib\\site-packages\\catalyst\\metrics\\_functional_metric.py:85\u001b[0m, in \u001b[0;36mFunctionalBatchMetric.update\u001b[1;34m(self, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    Calculate metric and update average metric\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m        custom metric\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditive_metric\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mfloat\u001b[39m(value), batch_size)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GSU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GSU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "# This will not work as this is not a classification. \n",
    "# We can try to use sklearn.metrics.score as the metric function and the respective metric key \n",
    "\n",
    "\n",
    "import torch\n",
    "from catalyst import metrics\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "metric = metrics.FunctionalBatchMetric(\n",
    "    metric_fn=sklearn.metrics.accuracy_score,\n",
    "    metric_key=\"sk_accuracy\",\n",
    ")\n",
    "metric.reset()\n",
    "\n",
    "metric.update(batch_size=len(y_pred), y_pred=torch.Tensor(y_pred), y_true=torch.Tensor(y))\n",
    "metric.compute()\n",
    "\n",
    "metric.compute_key_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "accuracy_score() got an unexpected keyword argument 'topk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m metric \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mTopKMetric(\n\u001b[0;32m      9\u001b[0m     metric_function\u001b[38;5;241m=\u001b[39msklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maccuracy_score,\n\u001b[0;32m     10\u001b[0m     metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m metric\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m metric\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GSU\\lib\\site-packages\\catalyst\\metrics\\_topk_metric.py:57\u001b[0m, in \u001b[0;36mTopKMetric.update\u001b[1;34m(self, logits, targets)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits: torch\u001b[38;5;241m.\u001b[39mTensor, targets: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Update metric value with value for new data\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    and return intermediate metrics values.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m        list of metric@k values\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     values \u001b[38;5;241m=\u001b[39m [v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics):\n",
      "\u001b[1;31mTypeError\u001b[0m: accuracy_score() got an unexpected keyword argument 'topk'"
     ]
    }
   ],
   "source": [
    "# This will not work as this is not a classification. \n",
    "# We can try to use sklearn.metrics.score as the metric function and the respective metric key instead of accuracy_score just \n",
    "# like the above code block \n",
    "\n",
    "import torch\n",
    "from catalyst import metrics\n",
    "import sklearn.metrics\n",
    "\n",
    "metric = metrics.TopKMetric(\n",
    "    metric_function=sklearn.metrics.accuracy_score,\n",
    "    metric_name=\"sk_accuracy\",\n",
    "    topk=[1, 3]\n",
    ")\n",
    "metric.reset()\n",
    "\n",
    "metric.update(logits=torch.Tensor(y_pred), targets=torch.Tensor(y))\n",
    "metric.compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
